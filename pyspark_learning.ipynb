{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a45bf9",
   "metadata": {},
   "source": [
    "# Problem Statement: We need to be able to import products from a CSV file and into a database. There are half a million product details to be imported into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "authorized-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4beb8792",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/02 19:53:09 WARN Utils: Your hostname, Admins-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.2.5 instead (on interface en0)\n",
      "22/06/02 19:53:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/pranavwadekar/.virtualenvs/venv-personal/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/pranavwadekar/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/pranavwadekar/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5540d1ea-d1e2-40cb-a772-b39b1a1f1f66;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.2.10 in central\n",
      "\tfound io.delta#delta-core_2.12;1.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 308ms :: artifacts dl 17ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;1.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.postgresql#postgresql;42.2.10 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5540d1ea-d1e2-40cb-a772-b39b1a1f1f66\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/9ms)\n",
      "22/06/02 19:53:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# creating spark config object with cores and degree of parallelism\n",
    "config = pyspark.SparkConf().setMaster(\"local[*]\").setAll([('spark.driver.extraJavaOptions',\n",
    "                                      \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED\"), \n",
    "                                     ('spark.jars.packages',\n",
    "                                      'org.postgresql:postgresql:42.2.10,io.delta:delta-core_2.12:1.1.0'),\n",
    "                                     (\"spark.sql.extensions\", \n",
    "                                      \"io.delta.sql.DeltaSparkSessionExtension\"),\n",
    "                                     (\"spark.sql.catalog.spark_catalog\", \n",
    "                                      \"org.apache.spark.sql.delta.catalog.DeltaCatalog\"),\n",
    "                                     ('spark.executor.cores', 4),\n",
    "                                     (\"spark.default.parallelism\", 4)])\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.appName('large_file_processor').config(conf=config).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "babd0653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c39f12ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# importing large csv file, also with nonNull sku values.\n",
    "df = spark.read.format(\"csv\").options(\n",
    "    path=\"/Users/pranavwadekar/Desktop/personal_projects/pyspark_learning/base_data/products.csv.gz\",\n",
    "    inferSchema=True, header=True).load().filter(col(\"sku\").isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a32f4f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                name|                 sku|         description|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|         Bryce Jones|pranav-lay-raise-...|Art community flo...|\n",
      "|John Robinson upd...|    cup-return-guess|Produce successfu...|\n",
      "|      Theresa Taylor|           step-onto|Choice should lea...|\n",
      "|        Roger Huerta| citizen-some-middle|Important fight w...|\n",
      "|        John Buckley|      term-important|Alone maybe educa...|\n",
      "|     Tiffany Johnson|       do-many-avoid|     Born tree wind.|\n",
      "|      Roy Golden DDS|     help-return-art|Pm daughter thous...|\n",
      "|        David Wright| listen-enough-check|Under its near. N...|\n",
      "|       Anthony Burch|    anyone-executive|I lose positive m...|\n",
      "|        Lauren Smith|  grow-we-decide-job|Smile yet fear so...|\n",
      "|          Bailey Cox|     suggest-similar|Peace happy lette...|\n",
      "|       Jeffrey Davis|     million-quality|See sea guy fire ...|\n",
      "|        Lisa Sanchez|   defense-money-but|Exactly surface m...|\n",
      "|       Jamie Johnson|   teacher-why-adult|Language somebody...|\n",
      "|  Christina Campbell|      point-find-pay|Company employee ...|\n",
      "|      Douglas Curtis|    floor-throughout|Push enter everyt...|\n",
      "|      Arthur Johnson|     various-mission|Myself none see s...|\n",
      "|        Brian Zamora|     everyone-budget|More particularly...|\n",
      "|        Jennifer May|    garden-style-top|Natural little ad...|\n",
      "|        Dylan Cortez|     address-eye-say|See realize offic...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e0df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd54e13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f88aba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.9 ms, sys: 3.58 ms, total: 7.48 ms\n",
      "Wall time: 6.94 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# writing records in postgres database with 4 parallel threads\n",
    "df1.write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .mode(\"Append\") \\\n",
    "  .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "  .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\") \\\n",
    "  .option(\"dbtable\", \"sample.products\") \\\n",
    "  .option(\"user\", \"sampleuser\") \\\n",
    "  .option(\"password\", \"samplepass\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4260a39",
   "metadata": {},
   "source": [
    "# UPSERT LOGIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6921ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for upserting we need to first get all the source data\n",
    "\n",
    "df2 = spark.read \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "  .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\") \\\n",
    "  .option(\"dbtable\", \"sample.products\") \\\n",
    "  .option(\"user\", \"sampleuser\") \\\n",
    "  .option(\"password\", \"samplepass\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533f9dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+\n",
      "|                name|                sku|         description|\n",
      "+--------------------+-------------------+--------------------+\n",
      "|     Candace Francis|  a-adult-situation|Fine month cold a...|\n",
      "|         Lance Baker|   a-agent-although|Speak ok executiv...|\n",
      "|         Jason Smith|   a-agree-job-hour|Choose recognize ...|\n",
      "|          Mark Payne|       a-among-beat|Alone drive feeli...|\n",
      "|        Jose Johnson|    a-appear-ground|Offer remain wife...|\n",
      "|      John Christian|    a-arrive-should|Another make gun ...|\n",
      "|      Jessica Wagner|a-art-song-economic|Garden sound pres...|\n",
      "|       Jeff Shepherd|a-assume-town-clear|Report right this...|\n",
      "|        Louis Taylor| a-at-away-girl-tax|Late husband inst...|\n",
      "|        Brent Willis|        a-authority|Animal law networ...|\n",
      "|         Mark Miller|   a-available-like|Land yet reach ri...|\n",
      "|       Carolyn Nixon|    a-available-per|Skin participant ...|\n",
      "|           Juan Kerr|  a-avoid-game-walk|Interview test sp...|\n",
      "|Dr. Jacob Dudley DDS| a-avoid-short-hand|Gas poor college ...|\n",
      "|       Mark Figueroa|     a-bag-material|Realize surface t...|\n",
      "|    Kimberly Griffin|     a-bed-property|Practice station ...|\n",
      "|       Jonathan Shaw|     a-better-agent|In total herself ...|\n",
      "|         Jimmy White|     a-blue-in-upon|Similar wear rath...|\n",
      "|       Teresa Golden|a-both-each-out-bad|Especially doctor...|\n",
      "|   Timothy Gutierrez|a-break-soon-future|Six home control....|\n",
      "+--------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2ee9253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# convert the data into delta format because we need delta for ACID support\n",
    "\n",
    "df2.write.format(\"delta\").mode(\"OverWrite\").option(\"overwriteSchema\", \"true\").save(\"/Users/pranavwadekar/Desktop/personal_projects/pyspark_learning/base_data/delta-table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a15b58a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from the delta table\n",
    "\n",
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, \n",
    "                                \"/Users/pranavwadekar/Desktop/personal_projects/pyspark_learning/base_data/delta-table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9265f3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.toDF().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18066066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add and modify some SKU's write your own csv.\n",
    "\n",
    "df4 = spark.read.format(\"csv\").options(\n",
    "      path=\"/Users/pranavwadekar/Desktop/personal_projects/pyspark_learning/base_data/updated_products.csv.gz\",\n",
    "      inferSchema=True, header=True).load().filter(col(\"sku\").isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d92e81b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 25:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                name|                 sku|         description|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|         Bryce Jones|pranav-lay-raise-...|Art community flo...|\n",
      "|John Robinson upd...|    cup-return-guess|Produce successfu...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 25:==========================================================(1 + 0) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "886edb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# upsert is equal to merge in spark, so we need to specify the upsert keys (merge keys) which SKU in our case.\n",
    "# when matched it means the data is updated so update all the columns if not then it is a new record, so insert the record\n",
    "\n",
    "deltaTable.alias(\"dest\").merge(\n",
    "    source = df4.alias(\"source\"),\n",
    "    condition = \"dest.sku = source.sku\"\n",
    "  ).whenMatchedUpdate(set =\n",
    "    {\n",
    "      \"sku\": \"source.sku\",\n",
    "      \"name\": \"source.name\",\n",
    "      \"description\": \"source.description\"\n",
    "    }\n",
    "  ).whenNotMatchedInsertAll() \\\n",
    "   .execute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab8ba13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# finally we will write it into database\n",
    "\n",
    "deltaTable.toDF().write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .mode(\"OverWrite\") \\\n",
    "  .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "  .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\") \\\n",
    "  .option(\"dbtable\", \"sample.products\") \\\n",
    "  .option(\"user\", \"sampleuser\") \\\n",
    "  .option(\"password\", \"samplepass\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b90980d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ff3409c",
   "metadata": {},
   "source": [
    "# Learnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "three-photographer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/29 15:39:21 WARN Utils: Your hostname, Admins-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)\n",
      "22/05/29 15:39:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/pranavwadekar/.virtualenvs/venv-personal/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/pranavwadekar/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/pranavwadekar/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-91610a2e-5a43-4cae-b13d-8c0f7ff0b67f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.2.10 in central\n",
      ":: resolution report :: resolve 143ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\torg.postgresql#postgresql;42.2.10 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-91610a2e-5a43-4cae-b13d-8c0f7ff0b67f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/6ms)\n",
      "22/05/29 15:39:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "config = pyspark.SparkConf().setAll([('spark.driver.extraJavaOptions',\n",
    "                                      \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED\"), \n",
    "                                     ('spark.jars.packages',\n",
    "                                      'org.postgresql:postgresql:42.2.10')])\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.appName('test').config(conf=config).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e183781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.driver.extraJavaOptions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b23882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "czech-candy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").options(\n",
    "    path=\"/Users/pranavwadekar/Desktop/personal_projects/pyspark_learning/base_data/survey.csv\",\n",
    "    inferSchema=True, header=True).load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "psychological-priest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+\n",
      "|                name|                sku|         description|\n",
      "+--------------------+-------------------+--------------------+\n",
      "|         Bryce Jones| lay-raise-best-end|Art community flo...|\n",
      "|       John Robinson|   cup-return-guess|Produce successfu...|\n",
      "|      Theresa Taylor|          step-onto|Choice should lea...|\n",
      "|Often stuff profe...|               null|                null|\n",
      "|        Roger Huerta|citizen-some-middle|Important fight w...|\n",
      "|        John Buckley|     term-important|Alone maybe educa...|\n",
      "|Want benefit mana...|               null|                null|\n",
      "|     Tiffany Johnson|      do-many-avoid|     Born tree wind.|\n",
      "|Boy marriage begi...|               null|                null|\n",
      "|Certain throw exe...|               null|                null|\n",
      "|      Roy Golden DDS|    help-return-art|Pm daughter thous...|\n",
      "|Process eat emplo...|               null|                null|\n",
      "|Increase author w...|               null|                null|\n",
      "|        David Wright|listen-enough-check|Under its near. N...|\n",
      "|Hospital upon suf...|               null|                null|\n",
      "|       Anthony Burch|   anyone-executive|I lose positive m...|\n",
      "|        Lauren Smith| grow-we-decide-job|Smile yet fear so...|\n",
      "|          Bailey Cox|    suggest-similar|Peace happy lette...|\n",
      "|May serious profe...|               null|                null|\n",
      "|       Jeffrey Davis|    million-quality|See sea guy fire ...|\n",
      "+--------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "developmental-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = df.repartition(7, 'Country')#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exotic-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c78741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.select('Gender', 'treatment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "735d8b67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|Gender|treatment|\n",
      "+------+---------+\n",
      "|Female|      Yes|\n",
      "|     M|       No|\n",
      "|  Male|       No|\n",
      "|  Male|      Yes|\n",
      "|  Male|       No|\n",
      "|  Male|       No|\n",
      "|Female|      Yes|\n",
      "|     M|       No|\n",
      "|Female|      Yes|\n",
      "|  Male|       No|\n",
      "|  Male|      Yes|\n",
      "|  male|       No|\n",
      "|female|      Yes|\n",
      "|  Male|       No|\n",
      "|  Male|       No|\n",
      "|female|      Yes|\n",
      "|  Male|      Yes|\n",
      "|  Male|      Yes|\n",
      "|  male|       No|\n",
      "|  Male|       No|\n",
      "+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "crude-allah",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# applying transformations\n",
    "\n",
    "df2 = df1.select('Gender', \n",
    "                 (when(df1.treatment=='Yes', 1).otherwise(0).alias('All-Yes')),\n",
    "                 (when(df1.treatment=='No', 1).otherwise(0).alias('All-No')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "017fd53a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|Gender|All-Yes|All-No|\n",
      "+------+-------+------+\n",
      "|Female|      1|     0|\n",
      "|     M|      0|     1|\n",
      "|  Male|      0|     1|\n",
      "|  Male|      1|     0|\n",
      "|  Male|      0|     1|\n",
      "|  Male|      0|     1|\n",
      "|Female|      1|     0|\n",
      "|     M|      0|     1|\n",
      "|Female|      1|     0|\n",
      "|  Male|      0|     1|\n",
      "|  Male|      1|     0|\n",
      "|  male|      0|     1|\n",
      "|female|      1|     0|\n",
      "|  Male|      0|     1|\n",
      "|  Male|      0|     1|\n",
      "|female|      1|     0|\n",
      "|  Male|      1|     0|\n",
      "|  Male|      1|     0|\n",
      "|  male|      0|     1|\n",
      "|  Male|      0|     1|\n",
      "+------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e0c9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count\n",
    "\n",
    "# df3 = df2.groupBy('Gender').agg(sum('All-Yes'), sum('All-No'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1a2884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing sql over dataframe, first need to create a view\n",
    "\n",
    "df.createOrReplaceTempView('someTemp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "315cb4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout = spark.sql(\n",
    "  \"\"\"select gender, sum(yes) sum_yes, sum(no) sum_no \n",
    "                            from (select case when lower(trim(gender)) in ('male','m','male-ish','maile','mal','male (cis)',\n",
    "                                                                           'make','male ','man','msle','mail','malr','cis man',\n",
    "                                                                           'cis male') then 'Male' \n",
    "                                              when lower(trim(gender)) in ('cis female','f','female','woman','femake','female ',\n",
    "                                                                           'cis-female/femme','female (cis)','femail') then 'Female'\n",
    "                                              else 'Transgender' \n",
    "                                         end as gender,\n",
    "                                         case when treatment == 'Yes' then 1 else 0 end as yes,\n",
    "                                         case when treatment == 'No' then 1 else 0 end as no\n",
    "                                   from someTemp) \n",
    "                                   where gender != 'Transgender'\n",
    "                                   group by gender\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e2014b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|gender|sum_yes|sum_no|\n",
      "+------+-------+------+\n",
      "|Female|    170|    77|\n",
      "|  Male|    450|   541|\n",
      "+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfout.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31e6e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing results to dataframe\n",
    "\n",
    "dfout.write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "  .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\") \\\n",
    "  .option(\"dbtable\", \"sample.surveys_aggr\") \\\n",
    "  .option(\"user\", \"sampleuser\") \\\n",
    "  .option(\"password\", \"samplepass\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9fecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "222bc507",
   "metadata": {},
   "source": [
    "# Writing files in different format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bca98ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write. \\\n",
    "    format(\"parquet\"). \\\n",
    "    mode(\"overwrite\"). \\\n",
    "    save(\"/Users/pranavwadekar/Desktop/personal_projects/pyspark_learning/base_data/parquet_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dc6caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"mode\", \"failfast\") \\\n",
    "  .load(\"/Users/pranavwadekar/Desktop/personal_projects/pyspark_learning/base_data/parquet_data\")\n",
    "\n",
    "df.write \\\n",
    "  .format(\"json\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save(\"/Users/pranavwadekar/Desktop/personal_projects/pyspark_learning/base_data/json_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e5afb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
