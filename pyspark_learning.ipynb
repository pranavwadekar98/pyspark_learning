{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "507ac82e",
   "metadata": {},
   "source": [
    "# Problem Statement: We need to be able to import products from a CSV file and into a database. There are half a million product details to be imported into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "authorized-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22fe482a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/29 17:52:13 WARN Utils: Your hostname, Admins-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)\n",
      "22/05/29 17:52:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/pranavwadekar/.virtualenvs/venv-personal/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/pranavwadekar/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/pranavwadekar/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c5dac101-408b-46e7-b893-f7093a50dd66;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.2.10 in central\n",
      ":: resolution report :: resolve 132ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\torg.postgresql#postgresql;42.2.10 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c5dac101-408b-46e7-b893-f7093a50dd66\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/6ms)\n",
      "22/05/29 17:52:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# creating spark config object with cores and degree of parallelism\n",
    "config = pyspark.SparkConf().setMaster(\"local[*]\").setAll([('spark.driver.extraJavaOptions',\n",
    "                                      \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED\"), \n",
    "                                     ('spark.jars.packages',\n",
    "                                      'org.postgresql:postgresql:42.2.10'),\n",
    "#                                      ('spark.executor.memory', '8G'),\n",
    "                                     ('spark.executor.cores', 4),\n",
    "                                     (\"spark.default.parallelism\", 4)])\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.appName('large_file_processor').config(conf=config).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad818297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# importing large csv file, also with nonNull sku values.\n",
    "df = spark.read.format(\"csv\").options(\n",
    "    path=\"/Users/pranavwadekar/Desktop/personal_projects/pyspark_learning/base_data/products.csv.gz\",\n",
    "    inferSchema=True, header=True).load().filter(col(\"sku\").isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18919e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# writing records in postgres database\n",
    "\n",
    "df.write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .mode(\"Append\") \\\n",
    "  .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "  .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\") \\\n",
    "  .option(\"dbtable\", \"sample.products\") \\\n",
    "  .option(\"user\", \"sampleuser\") \\\n",
    "  .option(\"password\", \"samplepass\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ce001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb95f312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48316cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "052f2148",
   "metadata": {},
   "source": [
    "# Learnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "three-photographer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/29 15:39:21 WARN Utils: Your hostname, Admins-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)\n",
      "22/05/29 15:39:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/pranavwadekar/.virtualenvs/venv-personal/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/pranavwadekar/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/pranavwadekar/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-91610a2e-5a43-4cae-b13d-8c0f7ff0b67f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.2.10 in central\n",
      ":: resolution report :: resolve 143ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\torg.postgresql#postgresql;42.2.10 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-91610a2e-5a43-4cae-b13d-8c0f7ff0b67f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/6ms)\n",
      "22/05/29 15:39:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "config = pyspark.SparkConf().setAll([('spark.driver.extraJavaOptions',\n",
    "                                      \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED\"), \n",
    "                                     ('spark.jars.packages',\n",
    "                                      'org.postgresql:postgresql:42.2.10')])\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.appName('test').config(conf=config).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e250c457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.driver.extraJavaOptions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4c73e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "czech-candy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").options(\n",
    "    path=\"/Users/pranavwadekar/Desktop/personal_projects/pyspark_learning/base_data/survey.csv\",\n",
    "    inferSchema=True, header=True).load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "psychological-priest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+\n",
      "|                name|                sku|         description|\n",
      "+--------------------+-------------------+--------------------+\n",
      "|         Bryce Jones| lay-raise-best-end|Art community flo...|\n",
      "|       John Robinson|   cup-return-guess|Produce successfu...|\n",
      "|      Theresa Taylor|          step-onto|Choice should lea...|\n",
      "|Often stuff profe...|               null|                null|\n",
      "|        Roger Huerta|citizen-some-middle|Important fight w...|\n",
      "|        John Buckley|     term-important|Alone maybe educa...|\n",
      "|Want benefit mana...|               null|                null|\n",
      "|     Tiffany Johnson|      do-many-avoid|     Born tree wind.|\n",
      "|Boy marriage begi...|               null|                null|\n",
      "|Certain throw exe...|               null|                null|\n",
      "|      Roy Golden DDS|    help-return-art|Pm daughter thous...|\n",
      "|Process eat emplo...|               null|                null|\n",
      "|Increase author w...|               null|                null|\n",
      "|        David Wright|listen-enough-check|Under its near. N...|\n",
      "|Hospital upon suf...|               null|                null|\n",
      "|       Anthony Burch|   anyone-executive|I lose positive m...|\n",
      "|        Lauren Smith| grow-we-decide-job|Smile yet fear so...|\n",
      "|          Bailey Cox|    suggest-similar|Peace happy lette...|\n",
      "|May serious profe...|               null|                null|\n",
      "|       Jeffrey Davis|    million-quality|See sea guy fire ...|\n",
      "+--------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "developmental-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = df.repartition(7, 'Country')#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exotic-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c78741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.select('Gender', 'treatment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "529f3015",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|Gender|treatment|\n",
      "+------+---------+\n",
      "|Female|      Yes|\n",
      "|     M|       No|\n",
      "|  Male|       No|\n",
      "|  Male|      Yes|\n",
      "|  Male|       No|\n",
      "|  Male|       No|\n",
      "|Female|      Yes|\n",
      "|     M|       No|\n",
      "|Female|      Yes|\n",
      "|  Male|       No|\n",
      "|  Male|      Yes|\n",
      "|  male|       No|\n",
      "|female|      Yes|\n",
      "|  Male|       No|\n",
      "|  Male|       No|\n",
      "|female|      Yes|\n",
      "|  Male|      Yes|\n",
      "|  Male|      Yes|\n",
      "|  male|       No|\n",
      "|  Male|       No|\n",
      "+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "crude-allah",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# applying transformations\n",
    "\n",
    "df2 = df1.select('Gender', \n",
    "                 (when(df1.treatment=='Yes', 1).otherwise(0).alias('All-Yes')),\n",
    "                 (when(df1.treatment=='No', 1).otherwise(0).alias('All-No')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2710400a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|Gender|All-Yes|All-No|\n",
      "+------+-------+------+\n",
      "|Female|      1|     0|\n",
      "|     M|      0|     1|\n",
      "|  Male|      0|     1|\n",
      "|  Male|      1|     0|\n",
      "|  Male|      0|     1|\n",
      "|  Male|      0|     1|\n",
      "|Female|      1|     0|\n",
      "|     M|      0|     1|\n",
      "|Female|      1|     0|\n",
      "|  Male|      0|     1|\n",
      "|  Male|      1|     0|\n",
      "|  male|      0|     1|\n",
      "|female|      1|     0|\n",
      "|  Male|      0|     1|\n",
      "|  Male|      0|     1|\n",
      "|female|      1|     0|\n",
      "|  Male|      1|     0|\n",
      "|  Male|      1|     0|\n",
      "|  male|      0|     1|\n",
      "|  Male|      0|     1|\n",
      "+------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d609c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df3 = df2.groupBy('Gender').agg(sum('All-Yes'), sum('All-No'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "018395e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing sql over dataframe, first need to create a view\n",
    "\n",
    "df.createOrReplaceTempView('someTemp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b7a8796",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout = spark.sql(\n",
    "  \"\"\"select gender, sum(yes) sum_yes, sum(no) sum_no \n",
    "                            from (select case when lower(trim(gender)) in ('male','m','male-ish','maile','mal','male (cis)',\n",
    "                                                                           'make','male ','man','msle','mail','malr','cis man',\n",
    "                                                                           'cis male') then 'Male' \n",
    "                                              when lower(trim(gender)) in ('cis female','f','female','woman','femake','female ',\n",
    "                                                                           'cis-female/femme','female (cis)','femail') then 'Female'\n",
    "                                              else 'Transgender' \n",
    "                                         end as gender,\n",
    "                                         case when treatment == 'Yes' then 1 else 0 end as yes,\n",
    "                                         case when treatment == 'No' then 1 else 0 end as no\n",
    "                                   from someTemp) \n",
    "                                   where gender != 'Transgender'\n",
    "                                   group by gender\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "270556fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|gender|sum_yes|sum_no|\n",
      "+------+-------+------+\n",
      "|Female|    170|    77|\n",
      "|  Male|    450|   541|\n",
      "+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfout.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88c147c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing results to dataframe\n",
    "\n",
    "df.write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .mode(\"OverWrite\") \\\n",
    "  .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "  .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\") \\\n",
    "  .option(\"dbtable\", \"sample.surveys_aggr\") \\\n",
    "  .option(\"user\", \"sampleuser\") \\\n",
    "  .option(\"password\", \"samplepass\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8e114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98ea5ba3",
   "metadata": {},
   "source": [
    "# Writing files in different format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f5ac34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write. \\\n",
    "    format(\"parquet\"). \\\n",
    "    mode(\"overwrite\"). \\\n",
    "    save(\"/Users/pranavwadekar/Desktop/personal_projects/pyspark_learning/base_data/parquet_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fd97bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"mode\", \"failfast\") \\\n",
    "  .load(\"/Users/pranavwadekar/Desktop/personal_projects/pyspark_learning/base_data/parquet_data\")\n",
    "\n",
    "df.write \\\n",
    "  .format(\"json\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save(\"/Users/pranavwadekar/Desktop/personal_projects/pyspark_learning/base_data/json_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c9653a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
